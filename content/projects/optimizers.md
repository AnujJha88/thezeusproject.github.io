### Project: Toy Optimizers on a Simple Function

<div class="progress-bar-container">
  <div class="progress-bar" style="width: 0%;"></div>
</div>

- [ ] **Task 1: Setup the environment**
  - [ ] Install Python, NumPy, and Matplotlib.
  - [ ] Set up a virtual environment or Jupyter notebook.
- [ ] **Task 2: Define the objective function**
  - [ ] Use a simple quadratic function: f(x) = x² + 4x + 4.
- [ ] **Task 3: Implement optimizers**
  - [ ] Gradient Descent
  - [ ] Stochastic Gradient Descent
  - [ ] Momentum
- [ ] **Task 4: Run optimization**
  - [ ] Track loss at each iteration.
  - [ ] Save parameter updates across time.
- [ ] **Task 5: Visualize the results**
  - [ ] Plot the loss vs iterations for each optimizer.
  - [ ] Plot trajectory on the function curve.

---

### Project: Optimizer Benchmark with Logistic Regression

<div class="progress-bar-container">
  <div class="progress-bar" style="width: 0%;"></div>
</div>

- [ ] **Task 1: Load and prepare dataset**
  - [ ] Use a binary classification dataset (e.g., sklearn’s `make_classification`).
  - [ ] Normalize features and split train/test.
- [ ] **Task 2: Implement logistic regression**
  - [ ] Write model using NumPy.
  - [ ] Compute loss using binary cross-entropy.
  - [ ] Derive gradients manually.
- [ ] **Task 3: Implement optimizers**
  - [ ] Gradient Descent
  - [ ] Momentum
  - [ ] RMSProp
  - [ ] Adam
- [ ] **Task 4: Train and track**
  - [ ] Run each optimizer for N epochs.
  - [ ] Log loss and accuracy over time.
- [ ] **Task 5: Analyze performance**
  - [ ] Plot comparisons (loss, accuracy curves).
  - [ ] Note convergence speed and stability.

---

### Project: Neural Network Optimizer Showdown

<div class="progress-bar-container">
  <div class="progress-bar" style="width: 0%;"></div>
</div>

- [ ] **Task 1: Prepare environment**
  - [ ] Use PyTorch or TensorFlow.
  - [ ] Load a small dataset (e.g., MNIST or Fashion-MNIST).
- [ ] **Task 2: Build a small neural network**
  - [ ] Use an MLP or simple CNN.
  - [ ] Modularize the training loop to switch optimizers easily.
- [ ] **Task 3: Add optimizers**
  - [ ] SGD
  - [ ] SGD + Momentum
  - [ ] Adam
  - [ ] AdamW
  - [ ] RMSProp
  - [ ] AdaGrad
  - [ ] NAdam
  - [ ] Lookahead
- [ ] **Task 4: Run training and evaluate**
  - [ ] Train each optimizer for the same number of epochs.
  - [ ] Record accuracy and loss.
- [ ] **Task 5: Create a leaderboard**
  - [ ] Summarize metrics across optimizers.
  - [ ] Comment on which optimizer performs best and why.
